# Serving a Model from OpenShift AI
:imagesdir: ../assets/images
:sectnums:

In this section, we'll deploy our trained Rock, Paper, Scissors game model using the Model Server in OpenShift AI. We'll create a Data Connection to access the model artifact and then set up a Model Server to expose the model as an API endpoint. This will enable seamless integration with the RoShambo game application when deployed.

## Deploy the Model

To deploy the model, you trained in the previous section using pipelines, click on the *Actions* button and select *Deploy*:

image::ai-openshit-deploy-drop-button.png[Deploy dropdown menu]

Then select as target environment your workspace:

[source,text,role="copypaste",subs=attributes+]
----
wksp-{user}
----

Then change the model name to:

[source,text,role="copypaste",subs=attributes+]
----
yolo-rps
----

Select *OpenVINO Model Server* as *Serving runtime*.

Select *onnx-1* as *Model framework*.

image::openshift-ai-target-runtime.png[First part of the configuration]

Moreover, select to create a *Model route*, but unselect the *Requier token authentication*.

image::openshift-ai-model-route.png[Second part of the configuration]

Finally verify the *Source model location* is pointing to the correct model location:

image::openshift-ai-model-location.png[Third part of the configuration]

Push the *Deploy* button to start the process of inferencing the model.

Push the *Data Science Project* section on the left menu, select the _wksp-{user}_ Project.

Then go to *models*, and you'll see the inferencing model, with the current status.
Wait till it is green.

If you click on the _Internal and External endpoints details_ you'll see the URLs to access the inferenced model either from the cluster or from outside the cluster:

image::openshift-ai-inferencing-model.png[Inferencing Model]

## Next Steps

Now that we have deployed the RoShambo game model using the Model Server in OpenShift AI, we can integrate it with the game application to enable AI-powered gameplay. In the next section, we'll test the inferencing endpoint by querying the model with sample input data and verifying the predictions.

### OpenVINO Model Server

OpenVINO Model Server (OVMS) is a high-performance system designed to serve machine learning models optimized by the OpenVINO toolkit. It enables efficient deployment of AI models in production environments, facilitating scalable and reliable inference across various applications.

== Key Features of OpenVINO Model Server

Flexible Deployment: OVMS supports deployment across diverse environments, including Docker containers, bare metal servers, and orchestration platforms like Kubernetes and OpenShift.

Standardized APIs: It provides REST and gRPC application programming interfaces, allowing client applications written in any programming language that supports these protocols to perform remote inference.

Efficient Resource Utilization: OVMS is optimized for both horizontal and vertical scaling, ensuring efficient use of computational resources to meet varying workload demands.

Secure Model Management: Models are stored either locally or in remote object storage services, and are not directly exposed to client applications, enhancing security and access control.

== Benefits of Using OpenVINO Model Server

Simplified Client Integration: Clients require minimal updates since the server abstracts the model details, providing a stable interface for inference requests.

Microservices Architecture Compatibility: OVMS is ideal for microservices-based applications, facilitating seamless integration into cloud environments and container orchestration systems.

Dynamic Model Management: When serving multiple models, OVMS allows for runtime configuration changes without needing to restart the server, enabling the addition, deletion, or updating of models on-the-fly.

For a quick start with OpenVINO Model Server, refer to the official documentation's link:https://github.com/openvinotoolkit/model_server[*quickstart guide*]. 