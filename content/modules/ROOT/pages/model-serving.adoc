# Serving a Model from OpenShift AI
:imagesdir: ../assets/images
:sectnums:

In this section, we'll deploy our trained RoShambo game model using the Model Server in OpenShift AI. We'll create a Data Connection to access the model artifact and then set up a Model Server to expose the model as an API endpoint. This will enable seamless integration with the RoShambo game application when deployed.

## Create a Data Connection to the Model

First, we need to create a Data Connection to the model artifact in the OpenShift AI environment. This will allow the Model Server to access the model and serve it as an API endpoint. We will create another data connection similar to the pipelines one, but this time for the model artifact.

TIP: OpenShift AI supports various storage options, including S3-compatible object storage, allowing you to store and retrieve your model artifacts seamlessly.

Navigate to the `game-{user}` data science project in OpenShift AI, and click on *Add data connection* from the Data connections section.

image::openshift-ai-add-data-connection.png[Add Data Connection]

Please copy & paste from the right side of the table, and when ready, click on the *Create* button.

[cols="1,1"] 
|===
| *Name*
| Shared Minio: Model
| *Access Key*
| minio
| *Secret Key*
| minio123
| *Endpoint*
| http://minio.ic-shared-minio.svc.cluster.local:9000
| *Region*
| none
| *Bucket*
| models
|===

image::openshift-ai-data-connection-created.png[Data Connection Created]

## Create a Model Server

Next, we'll create a Model Server to serve the RoShambo game model as an API endpoint. This will allow the game application to interact with the model and make predictions based on the player's moves. From the Models and model servers section, hit *Add model server*.

TIP: When you create a Model Server in OpenShift AI, it automatically provisions the necessary infrastructure and resources to host and serve your model. Under the hood, OpenShift AI uses Kubernetes and KServe to orchestrate the deployment and scaling of the model server containers.

image::openshift-ai-add-model-server.png[Add Model Server]

Please copy & paste from the right side of the table, and when ready, click on the *Create* button. Note, you only have to select these options, the rest can be left as default:

[cols="1,1"] 
|===
| *Model server name*
| Model Server
| *Serving runtime*
| OpenVINO Model Server
|===

image::openshift-ai-model-server-created.png[Model Server Created]

## Deploy the Model

Now that the Model Server is created, we can deploy the RoShambo game model to the server. Click on the *Deploy model* button to the right of the Model Server.

image::openshift-ai-deploy-model.png[Deploy Model]

Please copy & paste from the right side of the table, and when ready, click on the *Create* button. Note, you only have to select these options, the rest can be left as default:

[cols="1,1"] 
|===
| *Model name*
| RPS
| *Model server*
| Model Server
| *Model framework*
| onnx-1
| *Existing data connection*
| Shared Minio - Model
| *Path*
| rps/
|===

image::openshift-ai-model-deployed.png[Model Deployed]

If the model is successfully deployed you will see its status as green after a minute or two. This inference server is now exposed in the cluster we're working on.

image::openshift-ai-model-deployed-status.png[Model Deployed Status]

TIP: The API endpoint provided by the Model Server follows a standard format, typically accepting input data (such as images or text) via `GRPC`, `REST`, and `HTTP` requests and returning the model's predictions in the response. This allows easy integration with client applications or services that need to consume the model's predictions.

## Next Steps

Now that we have deployed the RoShambo game model using the Model Server in OpenShift AI, we can integrate it with the game application to enable AI-powered gameplay. In the next section, we'll test the inferencing endpoint by querying the model with sample input data and verifying the predictions.

### OpenVINO Model Server

OpenVINO Model Server (OVMS) is a high-performance system designed to serve machine learning models optimized by the OpenVINO toolkit. It enables efficient deployment of AI models in production environments, facilitating scalable and reliable inference across various applications.

== Key Features of OpenVINO Model Server

Flexible Deployment: OVMS supports deployment across diverse environments, including Docker containers, bare metal servers, and orchestration platforms like Kubernetes and OpenShift.

Standardized APIs: It provides REST and gRPC application programming interfaces, allowing client applications written in any programming language that supports these protocols to perform remote inference.

Efficient Resource Utilization: OVMS is optimized for both horizontal and vertical scaling, ensuring efficient use of computational resources to meet varying workload demands.

Secure Model Management: Models are stored either locally or in remote object storage services, and are not directly exposed to client applications, enhancing security and access control.

== Benefits of Using OpenVINO Model Server

Simplified Client Integration: Clients require minimal updates since the server abstracts the model details, providing a stable interface for inference requests.

Microservices Architecture Compatibility: OVMS is ideal for microservices-based applications, facilitating seamless integration into cloud environments and container orchestration systems.

Dynamic Model Management: When serving multiple models, OVMS allows for runtime configuration changes without needing to restart the server, enabling the addition, deletion, or updating of models on-the-fly.

For a quick start with OpenVINO Model Server, refer to the official documentation's quickstart guide. https://github.com/openvinotoolkit/model_server