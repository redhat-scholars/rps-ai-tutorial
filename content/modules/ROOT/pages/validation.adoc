# Validating the Model Deployment
:imagesdir: ../assets/images
:sectnums:

In this section, we'll validate and test our deployed RoShambo game model using the Jupyter Hub environment in OpenShift AI. 

## Testing the Model Deployment

Go back to the tab where you opened the Jupyter Notebook (in case you closed go to *Data Science Projects*, *wksp-{user}*, *workbenches* and open again)

At the jupyter notebook, open the `3-yolo-rps-request-model.ipynb` and modify the following lines:

[source,python,role="copypaste",subs=attributes+]
----
KSERVE_URL = "https://yolo-rps-wksp-{user}.{openshift_cluster_ingress_domain}"
MODEL_NAME = "yolo-rps"
----

TIP: You find the inference API URL and the model name in the Model Serving section as discussed in the previous section:
image::openshift-ai-inferencing-model-2.png[Inferencing Model]

Let's run the notebook to test the deployed model. Hit the *Play* button in the upper hand area of the notebook to run the cells, firstly, to install the required libraries and then to set some variables for the inference URL and model/input names.
Then load the image and sent it usingan HTTP call to the model.
Finally, shows the result of the image detecting the correct shape, where in the first version, we got it an error:

image::openshift-ai-shape-v2.png[Jupyter Notebook Validation]

TIP: The notebook loads the hand gesture images from a specified directory and preprocesses them to match the input requirements of the model. This involves resizing the images to the expected dimensions and normalizing the pixel values.

## Next Steps

After running the notebook, you should see the model's predictions for the three hand gesture images. If the model correctly identifies the hand gestures, then the deployment is successful. In the next section, we'll deploy the game application using OpenShift GitOps and configure it to communicate with the model server for real-time hand gesture classification.