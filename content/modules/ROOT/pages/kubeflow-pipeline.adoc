# Working with Kubeflow Pipelines
:imagesdir: ../assets/images
:sectnums:

## Kubeflow Pipelines

link:https://www.kubeflow.org/docs/components/pipelines/overview/[Kubeflow Pipelines (KFP),window='_blank'] is a platform for building and deploying portable and scalable machine learning (ML) workflows using containers on Kubernetes-based systems.

KFP enables data scientists and machine learning engineers to author end-to-end ML workflows natively in Python. A pipeline is a definition of a workflow that composes one or more components together to form a computational directed acyclic graph (DAG). At runtime, each component execution corresponds to a single container execution, which may create ML artifacts.

In this lab, you'll find the Kubeflow pipeline and components in the `roshambo-notebooks/pipelines` dir as follows:

* Pipeline: `training_pipeline.py`
* Components: `fetch_data.py`, `train_model.py`, `evaluate_model.py`, `save_model.py`

Open the `training_pipeline.py`.

image::openshift-ai-jupyter-notebook-run3-pipeline1.png[Jupyter Notebook Pipeline]

Scroll down in the code and replace the following code with your username and cluster domain:

[source,python,role="copypaste",subs=attributes+]
----
if __name__ == '__main__':
    metadata = {
        "model_name": "yolov11",
        "version": "v2",
        "user": "{user}",
        "cluster_domain": "{openshift_cluster_ingress_domain}",
    }
----

image::openshift-ai-jupyter-notebook-run3-pipeline2.png[Jupyter Notebook Pipeline User]


Now go to the top-left of the notebook menu bar and click to the play icon to start the pipeline.

image::openshift-ai-jupyter-notebook-run3-pipeline3.png[Jupyter Notebook Pipeline]

The KFP is now starting on the cluster and you can monitor the progress the dashboard.
Go back to the  link:https://rhods-dashboard-redhat-ods-applications.{openshift_cluster_ingress_domain}[*OpenShift AI Dashboard*,role='params-link',window='_blank']. From the left-side menu, click to *Experiments* then *Experiments and runs*. From the list, you should see a *kfp-training* experiment running.
 

image::openshift-ai-jupyter-notebook-run3-pipeline5.png[Jupyter Notebook Pipeline]

Click to the Kubeflow pipeline to see all details.

image::openshift-ai-jupyter-notebook-run3-pipeline6.png[Jupyter Notebook Pipeline]

You should see now the graphical representation of the pipeline with all the components involved. 

Click to any of them to review status of logs. Note that the pipeline is running for a couple of minutes to finish.

image::openshift-ai-jupyter-notebook-run3-pipeline7.png[Jupyter Notebook Pipeline]

After the Kubeflow pipeline successful run, the new model has been pushed to the Model Registry that you can review.

Navigate to the left-side menu, go to *Model Registry* and click to the newly created *v2*.

image::openshift-ai-jupyter-notebook-run3-pipeline8.png[Jupyter Notebook Pipeline]

Review all details such as storage location and properties such as Precision and Recall.

image::openshift-ai-jupyter-notebook-run3-pipeline9.png[Jupyter Notebook Pipeline]


## Next Steps

With our model developed and trained, we're now ready to serve it using the Model Server in OpenShift AI. In the next section, we'll create a Model Server and test the served model.