# Setting up the Data Science Project
:linkattrs:
:window: _blank
:imagesdir: ../assets/images
:sectnums:

Let's begin this workshop by accessing our OpenShift AI instance and creating a new *Data Science Project*. This project will serve as a central workspace containing all assets related to your data science work—such as notebooks, data connections, model deployments, and more. Let’s get started and explore what OpenShift AI has to offer!

TIP: These technologies are based on open-source projects that Red Hat contributes to, such as link:https://jupyter.org/[Jupyter] for notebooks, link:https://www.kubeflow.org/[Kubeflow] for machine learning pipelines, and link:https://www.knative.dev/[Knative] for serverless model serving, just to name a few!

## Accessing OpenShift AI

To begin, select *Red Hat OpenShift AI* from the OpenShift Web Console application launcher, or by clicking on the following link: https://rhods-dashboard-redhat-ods-applications.{openshift_cluster_ingress_domain}[*OpenShift AI Dashboard*].

image::openshift-ai-launcher.png[OpenShift AI Launcher]

## Exploring OpenShift AI

Here's your launchpad to building and operationalizing AI/ML models on OpenShift. Let's briefly explore the different sections of the OpenShift AI Dashboard before we create our project.

image::openshift-ai-dashboard-view.png[OpenShift AI Dashboard]

### Applications

The applications section provides access to enabled applications that extend the capabilities of OpenShift AI such as link:https://jupyter.org/[Jupyter Hub,role='params-link',window='_blank'], link:https://www.anaconda.com/[Anaconda,role='params-link',window='_blank'], and link:https://www.starburst.io/[Starburst,role='params-link',window='_blank'].

image::openshift-ai-applications.png[OpenShift AI Applications]

### Data Science Projects

This area is where you can create and manage your data science projects. Today, we created the following 2 projects for you in advance.

* ArgoCD GitOps Namespace
* Workspace

image::openshift-ai-projects.png[OpenShift AI Projects] 

### Data Science Pipelines

This section is where you can create and manage your machine learning pipelines. These pipelines can be used to automate the end-to-end machine learning workflow, from data preparation to model training and deployment. Based on Kubeflow and Argo Workflows, these pipelines are portable and scalable.

image::openshift-ai-pipelines.png[OpenShift AI Pipelines]

### Model Serving

This area is where you can deploy and manage your machine learning models. Based on Istio and Knative, this serverless framework allows you to scale your models based on demand, and provides a consistent way to deploy and manage your models.

image::openshift-ai-model-serving.png[OpenShift AI Model Serving]

### Resources

This section provides access to resources such as documentation, tutorials, and more to help you get started with OpenShift AI. From data analysis to model deployment and monitoring, you'll find everything you need to build and operationalize AI/ML models on OpenShift.

image::openshift-ai-resources.png[OpenShift AI Resources]

## Working with Data Science Projects

Fantastic, we understand the basics of OpenShift AI and are ready to use our first data science project. Head to the *Data Science Projects* and select your already created project `wskp-{user}`.

image::openshift-ai-project-selection.png[Select Data Science Project]

Welcome to your first data science project! This is where you'll be spending most of your time in this workshop, building and operationalizing your AI/ML models. Let's start by creating a data connection where we'll be loading our data from.

image::openshift-ai-project-overview.png[Select Data Science Project]

### Data Connections

We've deployed an instance of MinIO S3-compatible object storage on the cluster. We'll use MinIO to store our data and create a connection to it in our project.

The data connection to MinIO to store the models and the pipelines have been already created for you, and you can review it by clicking to the "Connections" tab.

image::openshift-ai-connections.png[Create Data Connection]

### Creating a Workbench for Jupyter Notebooks

Speaking of notebooks, let's create a workbench where we can run our Jupyter Notebooks. Scroll up to the *Workbench* tab and click on the *Create workbench* button.

image::openshift-ai-create-workbench.png[Create Workbench]

For the workbench name, you can customize it, for ex. `My Workbench`, and select the *Image selection* to *Standard Data Science*. All else can remain as default.

image::openshift-ai-create-workbench-form.png[Create Workbench Form]

In the *Connections* section, click to *Attach existing connection*.

image::openshift-ai-create-workbench-connection.png[Create Workbench Connection]

Select *Storage* from the dropdown list.

image::openshift-ai-create-workbench-connection-storage.png[Create Workbench Connection]

Click to *Attach*.

image::openshift-ai-create-workbench-connection-storage-attach.png[Create Workbench Connection]

Then click to the *Create workbench* button.

You'll need to wait just a few minutes for the workbench to be created, but once it's ready, click the *Open* button to access the Jupyter Notebook environment.

NOTE: During this process, the notebook image is being pulled from the container registry, and pods are being created to run the notebook server.

Click to *Open* to start the workbench.

image::openshift-ai-workbench-ready.png[Workbench Ready]

You'll authenticate with your OpenShift user credentials ({user}, {password}), hit *Allow selected permissions* after to proceed, and finally land at the Jupyter Notebook environment.

image::openshift-ai-jupyter-notebook.png[Jupyter Notebook]

### Cloning the project repository

We have a repository with the code and data for this workshop. Let's clone it into our Jupyter Notebook environment. In the Jupyter Notebook environment, navigate to the *Git UI* in the left sidebar, and then click on the *Clone a Repository* button. 

image::openshift-ai-jupyter-notebook-git-ui.png[Jupyter Notebook Git UI]

Enter the repository URL from Gitea.

[source,sh,role="copypaste",subs=attributes+]
----
{gitea_console_url}/{user}/rps-game
----

image::openshift-ai-jupyter-notebook-clone-repo.png[Jupyter Notebook Clone Repository]

Click on the *Clone* button to clone the repository.


Go back to the file discovery and navigate to the `rps-game/roshambo-notebooks` folder to access the notebooks.

image::openshift-ai-jupyter-notebook-repo.png[Jupyter Notebook Repository]

Swich to the `kubecon-eu-2025` branch that we'll be working with today.

image::openshift-ai-checkeout-branch.png[Jupyter Notebook Checkout Repository]

IMPORTANT: Don't forget to switch the branch!


## Summary

We've successfully created a data science project, connected to MinIO, configured a pipeline server, and created a workbench to run our Jupyter Notebooks. We've also cloned the project repository into our Jupyter Notebook environment. Let's move on to the next section to start learn about the model we'll be using for the Rock-Paper-Scissors game.